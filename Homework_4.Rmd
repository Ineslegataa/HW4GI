---
title: "HW4-Group I"
author: "Cusma Fait, Masella, El Gataa"
date: "2024-1-7"
output: html_document
---

```{r setup, include=FALSE}
library(MASS)

knitr::opts_chunk$set(echo=TRUE)
set.seed(0)
```




## FSDS - Chapter 4


### Ex 4.42

Consider $n$ independent observations from a $N(\mu, \sigma^2)$ distribution.

(a) Focusing on $\mu$, find the likelihood function. Show that the log-likelihood function is a concave, parabolic function of $\mu$. Find the ML estimator $\hat \mu$.

(b) Considering $\sigma^2$ to be known, find the information using (i) equation (4.3), (ii) equation (4.4). Use them to show that the large-sample variance of $\hat \mu$ is $\sigma^2/n$. Explain why $\hat \mu$ is the minimum variance unbiased estimator.

(c) Show that the ML estimator $\hat \sigma$ of $\sigma$ is $\sqrt{\frac{1}{n}\Sigma_i(Y_i - \bar Y)^2}$. Show that the large-sample variance of $\hat \sigma$ is $\sigma^2/2n$.

**Solution**

(a)

First, we write down the likelihood function;
$$L(\mu, \sigma^2|y) = \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} exp \left(-\frac{(y_i-\mu)^2}{2 \sigma^2}\right)$$
Then we use the likelihood to obtain the log-likelihood;
$$l(\mu, \sigma^2|y) = \sum_{i=1}^n log \left(\frac{1}{\sqrt{2 \pi \sigma^2}} exp \left(-\frac{(y_i-\mu)^2}{2 \sigma^2} \right) \right)$$
$$= -\frac{1}{2}\sum_{i=1}^n \left(log(2 \pi \sigma^2) +  \frac{(y_i-\mu)^2}{ \sigma^2} \right)$$

$$= -\frac{1}{2} \left(n \space log(2 \pi \sigma^2) +\frac{1}{ \sigma^2} \sum_{i=1}^n (y_i-\mu)^2 \right) $$
We can see that the log-likelihood function is a parabolic function of $\mu$.

We now proceed by calculating the derivatives;

$$\frac{\partial l(\mu, \sigma|Y)}{\partial\mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (Y_i-\mu)
= n \frac{\mu - \bar Y}{\sigma^2}$$

By equating the derivative to $0$, we get the ML estimate of the mean 
$$n \frac{\hat \mu - \bar Y}{\sigma^2} = 0$$
$$\hat \mu = \frac{1}{n}\sum_{i=1}^n Y_i = \bar y$$
The ML estimator for the true mean is the sample mean.

$$\left( \frac{\partial}{\partial\mu} \right)^2 l(\mu, \sigma|Y) =  -\frac{n}{\sigma^2} < 0$$
Therefore, the log-likelihood function is also concave.


(b)

By assuming that we know $\sigma$, and by using the previous point, we can now calculate the Fisher expected information of the ML estimator $\hat \mu$ in two different ways:

$$
I(\mu|Y) 
= - \space E \left(\frac{\partial^2 l(\mu|Y)}{\partial \mu^2} \right)
= -E(-\frac{n}{\sigma^2}) 
= \frac{n}{\sigma^2}
$$
Similarly:

$$I(\mu|Y) = n \space E \left(\frac{\partial l(\mu|Y)}{\partial \mu} \right)^2$$
Since this estimator reaches the Cramér–Rao lower bound, it is considered a minimum variance estimator. 


(c)

We derive the log-likelihood from before in respect to the variance;
$$\frac{dl}{d(\sigma^2)} = -\frac{1}{2} \left( \frac{n}{\sigma^2} - \frac{1}{ \sigma^4} \Sigma_{i=1}^n (y_i-\mu)^2 \right) $$

By equating the derivative to 0 we get the ML estimator of the variance;
$$-\frac{1}{2} \left(\frac{n}{\hat \sigma^2} - \frac{1}{\hat \sigma^4} \sum_{i=1}^n (y_i-\mu)^2 \right) = 0$$
$$\hat \sigma^2 = \frac{1}{n}\sum_{i=1}^n (y_i-\bar y)^2$$
The ML estimator for the true standard deviation is the standard deviation of the sample. 

We can show that the large-sample variance of $\hat \sigma$ is $\sigma^2/2n$ with an example:

```{r 4_42_a}
sample_size <- 50
n_samples <- 1000

data <- c()
for (i in 1:n_samples){
  sample <- rnorm(sample_size)
  data[length(data)+1] <- sd(sample)
}
```

expected variance
```{r 4_42_b}
1/(2 * sample_size)
```

actual variance
```{r 4_42_c}
var(data) 
```



### Ex 4.62

For the bootstrap method, explain the similarity and difference between the true sampling distribution of $\hat \theta$ and the empirically-generated bootstrap distribution in terms of its center and its spread.

**Solution**

The following is an example that compares the distribution of $\hat \theta$ generated by sampling from the true distribution and by re-sampling the same sample multiple times with repetition (bootstrap).

```{r 4_62_a}
n <- 1000
sample_size <- 100
metric <- mean
data <- c()

for (i in 1:n) {
  y <- rnorm(sample_size)
  data[i] <- metric(y)
}

hist(data, nclass=30, main="sampling distribution of sample mean")
mean(data)
sd(data)
```

```{r 4_62_b}
y_boot <- rnorm(sample_size)
v_boot <- c()

for (i in 1:n) {
  indices <- sample.int(sample_size, sample_size, replace = TRUE)
  y <- y_boot[indices]
  v_boot[i] <- metric(y)
}

hist(v_boot, nclass=30, main="bootstrap of sample mean", col='palegreen3')
mean(v_boot)
sd(v_boot)
```

As we can see, since the sample used for the bootstrap was obtained by sampling the true distribution, both the center and the spread of the bootstrap distribution closely resemble those of the true sampling distribution of $\hat \theta$. 

This, however, is not true in general, as we can see in the following example.

```{r 4_62_c}
n <- 1000
sample_size <- 100
metric <- max
data <- c()

for (i in 1:n) {
  y <- rnorm(sample_size)
  data[i] <- metric(y)
}

hist(data, nclass=30, main="sampling distribution of sample max")
mean(data)
sd(data)
```

```{r 4_62_d}
y_boot <- rnorm(sample_size)
v_boot <- c()

for (i in 1:n) {
  indices <- sample.int(sample_size, sample_size, replace = TRUE)
  y <- y_boot[indices]
  v_boot[i] <- metric(y)
}

hist(v_boot, nclass=30, main="bootstrap of sample max", col='indianred')
mean(v_boot)
sd(v_boot)
```

In this example, the metric in question is the maximum value. Since this metric is so sensitive to the variety of values across the samples, the bootstrap method fails to capture the true spread and especially the shape of the distribution.



## FSDS - Chapter 8


### Ex 8.4

Refer to $Exercise$ $8.1$. Construct a classification tree, and prune strongly until the tree uses a single explanatory variable. Which crabs were predicted to have satellites? How does the proportion of correct predictions compare with the more complex tree in $Figure$  $8.2$?

**Solution**

...

```{r 8_4, echo=TRUE}

```

...



## LAB

Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i∼Exponential(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

- $\pi(\lambda)=Beta(4,2)$

- $\pi(\lambda)=Normal(1,2)$

- $\pi(\lambda)=Gamma(4,2)$ 

Now, compute your posterior as $\pi(\lambda|y) \space \propto \space L(\lambda;y)\pi(\lambda)$ for the selected prior. If your first choice was correct, you will be able to compute it analytically.

**Solution**


The choice of prior distribution depends on the nature of the problem and the parameter we're estimating, in this case the rate parameter $\lambda$ of an exponential distribution.
Let's look at all the options and see which one suits our needs better:

- $\beta(4,2)$: The beta distribution is defined on the interval $[0,1]$, so it's not suitable as a prior for $\lambda$, which can take any positive value.

- $N(1,2)$: The normal distribution is defined on the entire real line, so it allows negative values of $\lambda$, which doesn't make sense in this context.

- $\gamma(4,2)$: The gamma distribution is defined on the positive real line, making it a suitable choice for a rate parameter $\lambda$. It's often used as a prior for the rate parameter of an exponential distribution because it's conjugate to the exponential likelihood, meaning that the posterior distribution is also a Gamma distribution.

In fact, given the exponential likelihood and a gamma prior, the posterior distribution can be calculated analytically. If $y_1,...,y_n$ are the call lengths and $λ \sim \gamma(a,b)$, then the posterior distribution of $\lambda$ is given by

$$
\begin{split}
  \pi(\lambda|y) \space &\propto \space L(\lambda;y)\pi(\lambda;\alpha,\beta)\\
  &\propto (\lambda^ne^{-\lambda n \bar y}) \times (\lambda^{\alpha-1}e^{-\beta\lambda})\\
  &=\lambda^{n+\alpha-1}e^{-\lambda(n\bar y+\beta)}
\end{split}
$$
As we can see, the posterior distribution is recognised as a gamma distribution with parameters $\gamma(\alpha+n,\beta+n\bar y)$, where $\bar y$ is the mean of the call length $y_i$.



## ISLR - chapter 7


### exercises 7.9

...

**Solution**

...

```{r 7_9, echo=TRUE}

```

...



## GAM

This question is about using $gam$ for univariate smoothing, the advantages of penalized regression and weighting a smooth model fit. The $mcycle$ data in the $MASS$ package are a classic dataset in univariate smoothing, introduced in Silverman (1985). The data measure the acceleration of the rider’s head, against time, in a simulated motorcycle crash.

1. Plot the acceleration against time, and use $gam$ to fit a univariate smooth to the data, selecting the smoothing parameter by GCV (k of 30 to 40 is plenty for this example). Plot the resulting smooth, with partial residuals, but without standard errors.

2. Use $lm$ and $poly$ to fit a polynomial to the data, with approximately the same degrees of freedom as was estimated by $gam$. Use $termplot$ to plot the estimated polynomial and partial residuals. Note the substantially worse fit achieved by the polynomial, relative to the penalized regression spline fit.

3. It’s possible to overstate the importance of penalization in explaining the improvement of the penalized regression spline, relative to the polynomial. Use $gam$ to refit an un-penalized thin plate regression spline to the data, with basis dimension the same as that used for the polynomial, and again produce a plot for comparison with the previous two results.

4. Redo part 3 using an un-penalized cubic regression spline. You should find a fairly clear ordering of the acceptability of the results for the four models tried - what is it?

5. Now plot the model residuals against time, and comment.

6. Fit a linear model including a b-spline using the function bs on times and select a suitable degree and the knots position. Compare this model with the previous ones and comment.

**Solution**

1. 

```{r GAM.0, include=FALSE}
# Load the packages
library(MASS)
library(mgcv)
library(splines)
```

First of all, let's take a look to our dataset
```{r GAM.1.1, echo=TRUE}

# Load the mcycle data
data(mcycle)

# Plot the data
plot(mcycle$times, mcycle$accel, xlab = "Time (ms)", ylab = "Acceleration (g)",
     main = "Acceleration vs Time in a Simulated Motorcycle Accident")
```


```{r GAM.1.2, echo=TRUE}
# Fit the model
fit_gam <- gam(accel ~ s(times, k = 35), data = mcycle, method = "GCV.Cp")

# Print the summary of the model
summary(fit_gam)

# Plot the smooth
plot(fit_gam, residuals = TRUE, se = FALSE)
```

2. 

Taking a look to the summary from the previous point we can see that edf (estimated degrees of freedom) of the model was around 11, so we'll use it here

```{r GAM.2.1, echo=TRUE}
# Fit a polynomial to the data
fit_poly <- lm(accel ~ poly(times, degree =11, raw = TRUE), data = mcycle)

# Print the summary of the model
summary(fit_poly)

# Plot the estimated polynomial and partial residuals
termplot(fit_poly, partial.resid = TRUE, se = TRUE)
```

3. 

```{r GAM.3.1, echo=TRUE}
# Fit the model
fit_unpenalized <- gam(accel ~ s(times, k = 11, bs = "tp"), data = mcycle)

# Print the summary of the model
summary(fit_unpenalized)

# Plot the estimated spline
plot(fit_unpenalized, residuals = TRUE, se = FALSE)
```

4.
```{r GAM.4.1, echo=TRUE}
# Fit the model
fit_cubic <- gam(accel ~ s(times, k = 11, bs = "cr"), data = mcycle)

# Print the summary of the model
summary(fit_cubic)

# Plot the estimated spline
plot(fit_cubic, residuals = TRUE, se = FALSE)
```

5.

```{r GAM.5.1, echo=TRUE}
par(mfrow=c(2,2))  # 2 rows, 2 columns

# For the gam model
plot(mcycle$times, residuals(fit_gam), xlab = "Time (ms)", ylab = "Residuals",
     main = "Residuals vs Time for the GAM Model")

# For the polynomial model
plot(mcycle$times, residuals(fit_poly), xlab = "Time (ms)", ylab = "Residuals",
     main = "Residuals vs Time for the Polynomial Model")

# For the unpenalized thin plate regression spline model
plot(mcycle$times, residuals(fit_unpenalized), xlab = "Time (ms)", ylab = "Residuals",
     main = "Residuals vs Time for the Unpenalized Thin Plate Regression Spline Model")

# For the unpenalized cubic regression spline model
plot(mcycle$times, residuals(fit_cubic), xlab = "Time (ms)", ylab = "Residuals",
     main = "Residuals vs Time for the Unpenalized Cubic Regression Spline Model")
```

6.

```{r GAM.6.1, echo=TRUE}
par(mfrow=c(1,1)) 

# Fit a linear model with a B-spline basis
fit_bs <- lm(accel ~ bs(times, degree = 1, knots = c(13.5,21,32,40)), data = mcycle)

# Print the summary of the model
summary(fit_bs)

# Plot the estimated polynomial and partial residuals
termplot(fit_bs, partial.resid = TRUE, se = TRUE)

# Plot the residuals
plot(mcycle$times, residuals(fit_bs), xlab = "Time (ms)", ylab = "Residuals",
     main = "Residuals vs Time for the B-Spline Model")
```

